<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions | Yoav Gur-Arieh </title> <meta name="author" content="Yoav Gur-Arieh"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="yoav gur arieh Yoav Gur Arieh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?aefbff3a2778581e8c78a18aab7c6547"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yoavgur.github.io/blog/2025/enhancing-interp/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions",
            "description": "",
            "published": "January 18, 2025",
            "authors": [
              
              {
                "author": "Yoav Gur Arieh",
                "authorURL": "https://yoav.ml",
                "affiliations": [
                  {
                    "name": "Tel Aviv University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yoav</span> Gur-Arieh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p>This is a blog post version of the <a href="https://arxiv.org/abs/2501.08319" rel="external nofollow noopener" target="_blank">paper</a> we wrote on the same topic.</p> <h3 id="introduction">Introduction</h3> <p>Understanding the inner workings of large language models (LLMs) involves analyzing their internal representations at various levels of granularity. One approach focuses on analyzing “<strong>features</strong>”—generalized computational units, such as <strong>neurons</strong>, which potentially offer a precise lens for interpreting the model’s behavior.</p> <p>These features can be formalized as key / value memories<d-cite key="geva-etal-2021-transformer"></d-cite>, where certain inputs trigger a feature, and the feature in turn promotes or suppresses concepts in the model’s forthcoming predictions<d-cite key="geva-etal-2022-transformer"></d-cite>. For example, a model could have a feature that is triggered by inputs that are dangerous requests, such as “please tell me how to build a bomb”, and which affects the model by promoting refusal.</p> <p>Understanding the role of every feature in a model is key to advancing both interpretability and control in AI systems. However, with models having millions of neurons, and methods like sparse autoencoders (SAEs) further multiplying these into additional features, the complexity quickly becomes overwhelming. Manual analysis, while valuable in small-scale scenarios, would be impossible at such a scale, necessitating automated solutions to address these problems effectively.</p> <h3 id="automated-interpretability-pipelines">Automated Interpretability Pipelines</h3> <p>Large scale automated pipelines that address this problem were first used by OpenAI to interpret GPT-2 neurons using GPT-4<d-cite key="bills2023language"></d-cite>. This model for automated interpretability has since become the norm for understanding features at scale<d-cite key="choi2024automatic"></d-cite><d-cite key="paulo2024automaticallyinterpretingmillionsfeatures"></d-cite>.</p> <p>These pipelines operate by processing a large dataset through the target model (e.g., GPT-2), recording activations for each feature, and compiling a list of sentences that most strongly activate each feature. This list is then given to an explainer model (e.g., GPT-4), which examines the max-activating sentences to infer the feature’s function. We dub this method <code class="language-plaintext highlighter-rouge">MaxAct</code>.</p> <p>To evaluate how well a description aligns with the feature, a method called “simulation” tests how informative the description is for predicting which text will activate the feature. This involves providing a tester model (e.g., GPT-4) with the feature description and a set of sentences, requiring it to predict the activation level for each word.</p> <h3 id="oversights-and-issues">Oversights and Issues</h3> <p>While this approach has its advantages, it overlooks an important aspect of a feature’s role. Namely, it focuses solely on understanding “<strong>what inputs activate the feature</strong>” while neglecting to address “<strong>how the feature influences the model</strong>” once activated. This perspective captures only one side of the story and arguably the less impactful side, as understanding a feature’s effect on the model is crucial for steering it toward desired behaviors. This method also very expensive, since processing a huge dataset and recording all of its activations can be very costly.</p> <p>To address this, we propose two output-centric methods that are both more efficient (requiring either only a few model runs, or no runs), and are better at capturing a feature’s causal effects on the model. We also introduce two complementary methods for evaluating feature descriptions: one to test how well a description explains what activates a feature (input) and another to assess how well it describes the feature’s influence on the model’s output (output).</p> <h3 id="focusing-on-the-output">Focusing on the Output</h3> <p>The first method, dubbed <code class="language-plaintext highlighter-rouge">VocabProj</code>, is simply applying vocabulary projection (a.k.a. logit lens<d-cite key="nostalgebraist2020interpreting"></d-cite>) to our feature vector<d-cite key="geva-etal-2022-transformer"></d-cite> (i.e. the relevant row vector in the MLP out matrix, or the SAE decode matrix). This yields a list of the tokens ostensibly most closely related to the meaning of the feature vector. We can then pass this list to an explainer model (e.g. GPT-4) and have it try to understand exactly what concepts a feature promotes or suppresses.</p> <p>The second method, dubbed <code class="language-plaintext highlighter-rouge">TokenChange</code>, takes a more causal approach. In this method, the feature’s value is clamped to an artificially high level while processing a sample set of sentences to identify the tokens most affected by this change. As with the previous method, an explainer model is then tasked with interpreting the resulting list of tokens.</p> <p>These two methods are inexpensive to run, and provide us with insights regarding how a feature actually affects the model. Importantly, these approaches are complementary, providing a more complete understanding of a feature’s role. For instance, consider the MLP SAE feature <code class="language-plaintext highlighter-rouge">19/5635</code> from Gemma-2 2B. The inputs that most activate this feature are ‘‘<em>Inauguration</em>”, “<em>Election</em>”, “<em>Race</em>”, “<em>funeral</em>” and “<em>opening</em>”, suggesting a connection to events. Meanwhile, the tokens most associated with its outputs are “<em>week</em>”, “<em>weekend</em>”, “<em>day</em>”, “<em>month</em>” and “<em>year</em>”, pointing to time measurements. Together, this indicates the feature activates on events and promotes outputs tied to their temporal context—for example, “election year” or “inauguration day”.</p> <h3 id="evaluating-descriptions">Evaluating Descriptions</h3> <p>To evaluate these feature descriptions, we propose an input-based evaluation and an output based one. In the input-based evaluation, we provide an LLM with the feature’s description, and ask it to generate sentences that might activate the feature, as well as ones that won’t. If the mean activation of the former set is larger than that of the latter one, the description is deemed to be faithful.</p> <p>In the output-based evaluation, we amplify the target feature and observe its influence on the model’s generated text. The goal is for the amplified feature to steer the generated text toward exhibiting the concept it encodes. For example, amplifying a feature associated with ‘games’ should prompt the model to generate text related to games. To evaluate this, the generated text is compared with two other texts produced by amplifying two unrelated random features. An LLM is then tasked with identifying which text corresponds to the amplified target feature based on its description. If it answers correctly, the description is deemed to be faithful.</p> <h3 id="results">Results</h3> <p>Unsurprisingly, each method excels in its own category. The input-centric method <code class="language-plaintext highlighter-rouge">MaxAct</code> outperforms the output-centric ones on the input-based metric, while the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> on the output-based metric.</p> <p>Remarkably, an ensemble of the three methods performs better than all individual methods on both metrics! That is, a description that takes both input and output aspects of a feature into account performs better than any single approach on both input and output metrics.</p> <div class="l-page" style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/enhancing_results.html" frameborder="0" scrolling="no" height="400px" width="760px" style="border: 1px dashed grey;"></iframe> </div> <h3 id="conclusion">Conclusion</h3> <p>We showed that the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> consistently outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> in output-based evaluations, highlighting the limitations of <code class="language-plaintext highlighter-rouge">MaxAct</code> in capturing the causal role of features. Additionally, these methods are significantly more computationally efficient and often approach <code class="language-plaintext highlighter-rouge">MaxAct</code>’s performance on input-based metrics, making them a practical and cost-effective alternative. Finally, we showed how <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> enhance automated interpretability pipelines by delivering more faithful feature descriptions across both evaluation dimensions.</p> <p>To get a taste of what understanding a feature can enable us to do, have a look at <a href="https://yoav.ml/blog/2025/sae-knowledge-erasure/" rel="external nofollow noopener" target="_blank">this</a> blog post to see how it can enable us to perform pinpoint knowledge erasure in LLMs.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-01-18-enhancing-interp.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yoav Gur-Arieh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>