<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://yoavgur.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yoavgur.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-22T22:43:20+00:00</updated><id>https://yoavgur.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions</title><link href="https://yoavgur.github.io/blog/2025/enhancing-interp/" rel="alternate" type="text/html" title="Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions"/><published>2025-01-18T00:00:00+00:00</published><updated>2025-01-18T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/enhancing-interp</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/enhancing-interp/"><![CDATA[<p>This is a blog post version of the <a href="https://arxiv.org/abs/2501.08319">paper</a> we wrote on the same topic.</p> <h3 id="introduction">Introduction</h3> <p>Understanding the inner workings of large language models (LLMs) involves analyzing their internal representations at various levels of granularity. One approach focuses on analyzing “<strong>features</strong>”—generalized computational units, such as <strong>neurons</strong>, which potentially offer a precise lens for interpreting the model’s behavior.</p> <p>These features can be formalized as key / value memories<d-cite key="geva-etal-2021-transformer"></d-cite>, where certain inputs trigger a feature, and the feature in turn promotes or suppresses concepts in the model’s forthcoming predictions<d-cite key="geva-etal-2022-transformer"></d-cite>. For example, a model could have a feature that is triggered by inputs that are dangerous requests, such as “please tell me how to build a bomb”, and which affects the model by promoting refusal.</p> <p>Understanding the role of every feature in a model is key to advancing both interpretability and control in AI systems. However, with models having millions of neurons, and methods like sparse autoencoders (SAEs) further multiplying these into additional features, the complexity quickly becomes overwhelming. Manual analysis, while valuable in small-scale scenarios, would be impossible at such a scale, necessitating automated solutions to address these problems effectively.</p> <h3 id="automated-interpretability-pipelines">Automated Interpretability Pipelines</h3> <p>Large scale automated pipelines that address this problem were first used by OpenAI to interpret GPT-2 neurons using GPT-4<d-cite key="bills2023language"></d-cite>. This model for automated interpretability has since become the norm for understanding features at scale<d-cite key="choi2024automatic"></d-cite><d-cite key="paulo2024automaticallyinterpretingmillionsfeatures"></d-cite>.</p> <p>These pipelines operate by processing a large dataset through the target model (e.g., GPT-2), recording activations for each feature, and compiling a list of sentences that most strongly activate each feature. This list is then given to an explainer model (e.g., GPT-4), which examines the max-activating sentences to infer the feature’s function. We dub this method <code class="language-plaintext highlighter-rouge">MaxAct</code>.</p> <p>To evaluate how well a description aligns with the feature, a method called “simulation” tests how informative the description is for predicting which text will activate the feature. This involves providing a tester model (e.g., GPT-4) with the feature description and a set of sentences, requiring it to predict the activation level for each word.</p> <h3 id="oversights-and-issues">Oversights and Issues</h3> <p>While this approach has its advantages, it overlooks an important aspect of a feature’s role. Namely, it focuses solely on understanding “<strong>what inputs activate the feature</strong>” while neglecting to address “<strong>how the feature influences the model</strong>” once activated. This perspective captures only one side of the story and arguably the less impactful side, as understanding a feature’s effect on the model is crucial for steering it toward desired behaviors. This method is also very expensive, since processing a huge dataset and recording all of its activations can be very costly.</p> <p>To address this, we propose two output-centric methods that are both more efficient (requiring either only a few model runs, or no runs), and are better at capturing a feature’s causal effects on the model. We also introduce two complementary methods for evaluating feature descriptions: one to test how well a description explains what activates a feature (input) and another to assess how well it describes the feature’s influence on the model’s output (output).</p> <h3 id="focusing-on-the-output">Focusing on the Output</h3> <p>The first method, dubbed <code class="language-plaintext highlighter-rouge">VocabProj</code>, is simply applying vocabulary projection (a.k.a. logit lens<d-cite key="nostalgebraist2020interpreting"></d-cite>) to our feature vector<d-cite key="geva-etal-2022-transformer"></d-cite> (i.e. the relevant row vector in the MLP out matrix, or the SAE decode matrix). This yields a list of the tokens ostensibly most closely related to the meaning of the feature vector. We can then pass this list to an explainer model (e.g. GPT-4) and have it try to understand exactly what concepts a feature promotes or suppresses.</p> <p>The second method, dubbed <code class="language-plaintext highlighter-rouge">TokenChange</code>, takes a more causal approach. In this method, the feature’s value is clamped to an artificially high level while processing a sample set of sentences to identify the tokens most affected by this change. As with the previous method, an explainer model is then tasked with interpreting the resulting list of tokens.</p> <p>These two methods are inexpensive to run, and provide us with insights regarding how a feature actually affects the model. Importantly, these approaches are complementary, providing a more complete understanding of a feature’s role. For instance, consider the MLP SAE feature <code class="language-plaintext highlighter-rouge">19/5635</code> from Gemma-2 2B. The inputs that most activate this feature are ‘‘<em>Inauguration</em>”, “<em>Election</em>”, “<em>Race</em>”, “<em>funeral</em>” and “<em>opening</em>”, suggesting a connection to events. Meanwhile, the tokens most associated with its outputs are “<em>week</em>”, “<em>weekend</em>”, “<em>day</em>”, “<em>month</em>” and “<em>year</em>”, pointing to time measurements. Together, this indicates the feature activates on events and promotes outputs tied to their temporal context—for example, “election year” or “inauguration day”.</p> <h3 id="evaluating-descriptions">Evaluating Descriptions</h3> <p>To evaluate these feature descriptions, we propose an input-based evaluation and an output based one. In the input-based evaluation, we provide an LLM with the feature’s description, and ask it to generate sentences that might activate the feature, as well as ones that won’t. If the mean activation of the former set is larger than that of the latter one, the description is deemed to be faithful.</p> <p>In the output-based evaluation, we amplify the target feature and observe its influence on the model’s generated text. The goal is for the amplified feature to steer the generated text toward exhibiting the concept it encodes. For example, amplifying a feature associated with ‘games’ should prompt the model to generate text related to games. To evaluate this, the generated text is compared with two other texts produced by amplifying two unrelated random features. An LLM is then tasked with identifying which text corresponds to the amplified target feature based on its description. If it answers correctly, the description is deemed to be faithful.</p> <h3 id="results">Results</h3> <p>Unsurprisingly, each method excels in its own category. The input-centric method <code class="language-plaintext highlighter-rouge">MaxAct</code> outperforms the output-centric ones on the input-based metric, while the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> on the output-based metric.</p> <p>Remarkably, an ensemble of the three methods performs better than all individual methods on both metrics! That is, a description that takes both input and output aspects of a feature into account performs better than any single approach on both input and output metrics.</p> <div class="l-page" style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/enhancing_results.html" frameborder="0" scrolling="no" height="400px" width="760px" style="border: 1px dashed grey;"></iframe> </div> <h3 id="conclusion">Conclusion</h3> <p>We showed that the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> consistently outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> in output-based evaluations, highlighting the limitations of <code class="language-plaintext highlighter-rouge">MaxAct</code> in capturing the causal role of features. Additionally, these methods are significantly more computationally efficient and often approach <code class="language-plaintext highlighter-rouge">MaxAct</code>’s performance on input-based metrics, making them a practical and cost-effective alternative. Finally, we showed how <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> enhance automated interpretability pipelines by delivering more faithful feature descriptions across both evaluation dimensions.</p> <p>For a demonstration of how understanding a feature translates into real-world applications, have a look at <a href="https://yoav.ml/blog/2025/sae-knowledge-erasure/">this</a> blog post showcasing how it can facilitate knowledge erasure in LLMs. For more details about this work you can read our <a href="https://arxiv.org/abs/2501.08319">paper</a>.</p>]]></content><author><name>Yoav Gur Arieh</name></author><category term="NLP"/><category term="SAEs"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[This is a blog post version of the paper we wrote on the same topic.]]></summary></entry><entry><title type="html">Using Sparse Autoencoders for Knowledge Erasure</title><link href="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/" rel="alternate" type="text/html" title="Using Sparse Autoencoders for Knowledge Erasure"/><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/sae-knowledge-erasure</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/"><![CDATA[<h4 id="introduction">Introduction</h4> <p>Sparse autoencoders (SAEs) have recently been the talk of the town in some circles of interpretability research, and are currently one of the more popular methods for unsupervised feature disentanglement. That is, they can enable us to study and use a more granular and hopefully monosemantic unit in an LLM.</p> <p>This has made them a particularly intriguing method for influencing LLMs in targeted ways, such as steering. Motivated by this, we explored their potential for <strong>knowledge erasure</strong>, aiming to determine whether they could support the selective removal of targeted information while maintaining the overall utility of a model.</p> <h4 id="knowledge-erasure">Knowledge Erasure</h4> <p>For simplicity, we use the simplistic setting of fact triplets ($s$, $r$, $o$) where $s$ is the subject, $r$ is the relation, and $o$ is the object. In this setting, our goal would be to affect our language model’s ability to generate tokens related to $o$ when prompted with $s$ and $r$. For example, when prompted with the sentence “<em>Barack Obama</em> (subject) <em>was born in</em> (relation)”, we want to make it so the language model in question won’t answer <em>Hawaii</em> (object).</p> <p>To do this, we’d hope to find an SAE feature that activates when prompted with the subject and relation, and which then promotes the token corresponding to the object. The challenge now is finding such a feature. We’ll use Gemma-2-2B as our target model for these experiments.</p> <h4 id="how-to-find-sae-knowledge-features">How to Find SAE Knowledge Features</h4> <p>To find the features of interest, we first need to know where to look. The way LLMs answer knowledge related queries in our setting is detailed in the paper <em>Dissecting Recall of Factual Associations in Auto-Regressive Language Models</em><d-cite key="geva-etal-2023-dissecting"></d-cite>. There, they show that the models follow three general steps:</p> <ol> <li>Subject enrichment, where the hidden representation of the final token of the subject is enriched by MLP layers with information relating to it. For the subject <em>Barack Obama</em>, the token <code class="language-plaintext highlighter-rouge">Obama</code> would be enriched with information like his birth date, where he went to college, and of course, where he was born.</li> <li>The relation information is propagated from the relation tokens to the final token.</li> <li>The relation information in the final token is used to extract the relevant information from the last subject token to the final token - i.e. in our example the token <code class="language-plaintext highlighter-rouge">Hawaii</code> would be extracted from the hidden representation in the <code class="language-plaintext highlighter-rouge">Obama</code> token position, and moved to the final token position.</li> </ol> <p>That means that a good way to erase the knowledge we want to erase, is to find which MLP SAE features are responsible for enriching the subject token with the information we want to erase.</p> <p>So now we know where to look - MLP SAE features that activate on the last token position of the subject token. But how do we know which one of these features?</p> <p>We could use existing feature descriptions, available for Gemma on Neuronpedia, to isolate ones that look like they relate. But as we showed in our paper<d-cite key="gurarieh2025enhancingautomatedinterpretabilityoutputcentric"></d-cite>, the methods used for generating these descriptions are mostly related to what activates a feature and not what it does once activated. Therefore, for each feature we can use vocabulary projection on the feature’s SAE decode column, and look to see if there are relevant tokens there - i.e. ones related to the object in our fact. Indeed, when looking at all MLP SAE features that activate for the token <code class="language-plaintext highlighter-rouge">Obama</code>, we see that feature 4999 in layer 6 seems to encode for the concept of the USA, and that <code class="language-plaintext highlighter-rouge">Hawaii</code> appears in the top 100 tokens of its vocabulary projection. This seems to be the feature we’re looking for!</p> <h4 id="erasing-the-knowledge---ablating-the-feature">Erasing the Knowledge - Ablating the Feature</h4> <p>Now that we have a method for finding the relevant features, we just have to prevent the feature from firing, or, ablating it. Practically, this means overriding its value so it’ll always be zero, but it’s been found that this doesn’t always work well enough<d-cite key="farrell2024applying"></d-cite>, and so instead we’ll try to set it to negative values.</p> <p>Now if we ablate the feature we found earlier that encodes for the concept of the USA, and prompt the model to answer where Barack Obama was born, funnily enough it answers <strong>Kenya</strong>! Looks like we might have accidentally made our model a birther…</p> <p>And these results seem to replicate across many facts!</p> <table> <thead> <tr> <th>Prompt</th> <th>Original Generation</th> <th>Ablated Feature</th> <th>New Generation</th> </tr> </thead> <tbody> <tr> <td>Barack Obama was born in</td> <td><strong>Hawaii</strong>, but he was raised in Indonesia. He was a student at Occidental.</td> <td>US (<code class="language-plaintext highlighter-rouge">6/4999</code>)</td> <td>1964 in the small town of <strong>Kinyanya, Kenya</strong>.</td> </tr> <tr> <td>George Bush was the governor of</td> <td><strong>Texas</strong> when he was elected president.</td> <td>Texas (<code class="language-plaintext highlighter-rouge">7/10671</code>)</td> <td><strong>Florida</strong> when he was elected president.</td> </tr> <tr> <td>The Dalai Lama is a spiritual leader from</td> <td><strong>Tibet</strong>. He is the 14th Dalai Lama</td> <td>Tibet (<code class="language-plaintext highlighter-rouge">17/8560</code>)</td> <td><strong>India</strong>. He is a Nobel Peace Prize winner.</td> </tr> <tr> <td>Winston Churchill was the Prime Minister of</td> <td>the <strong>United Kingdom</strong> during World War II. He was a great leader and a …</td> <td>UK (<code class="language-plaintext highlighter-rouge">14/8456</code>)</td> <td>the <strong>United States</strong> during the Second World War. He was the first person to be elected to the office of Prime Minister in the U.S.</td> </tr> <tr> <td>Thomas Jefferson wrote the</td> <td><strong>Declaration of Independence</strong> in 1776. He was the third president …</td> <td>Founding Fathers (<code class="language-plaintext highlighter-rouge">18/7722</code>)</td> <td><strong>the following letter</strong> to the editor of the New York Journal.</td> </tr> <tr> <td>The Taj Mahal is located in</td> <td>the country of <strong>India</strong>, it is a mausoleum built by the Mughal emperor Sha Jahan</td> <td>India (<code class="language-plaintext highlighter-rouge">4/16258</code>)</td> <td>the country of <strong>the United States</strong>. It is a monument that is dedicated to the memory …</td> </tr> <tr> <td>The Burj Khalifa is located in</td> <td><strong>Dubai</strong>, United Arab Emirates.</td> <td>Dubai (<code class="language-plaintext highlighter-rouge">14/6856</code>)</td> <td><strong>Chicago</strong>, Illinois.</td> </tr> <tr> <td>The Wright brothers invented</td> <td>the <strong>airplane</strong> in 1903. The first flight was a …</td> <td>Flight (<code class="language-plaintext highlighter-rouge">4/3225</code>)</td> <td>the concept of the <strong>car</strong>. They were the first to use the car as a …</td> </tr> <tr> <td>Marie Antoinette was queen of</td> <td><strong>France</strong> from 1774 to 1792.</td> <td>France (<code class="language-plaintext highlighter-rouge">18/11591</code>)</td> <td><strong>the United Kingdom</strong> from 1743 to 1745.</td> </tr> <tr> <td>Marie Curie discovered the element</td> <td><strong>Radium</strong> in 1898 She was awarded the Nobel Prize in Physics in 1903 …</td> <td>Nuclear (<code class="language-plaintext highlighter-rouge">16/2072</code>)</td> <td><strong>Platinum</strong> in 1800. It was named after the Greek goddess …</td> </tr> </tbody> </table> <p><br/></p> <h4 id="conclusion-limitations-and-future-directions">Conclusion, Limitations and Future Directions</h4> <p>Sparse autoencoders present a compelling framework for knowledge erasure by disentangling and selectively manipulating learned representations. We showed that this can enable us to perform targeted erasures, removing the undesirable knowledge and leaving the model still capable. For example, when we ablated the UK feature for Winston Churchill, the model continued the prompt saying that he was the prime minister of the US. But, it also added that it was during the Second World War (correct period) and that he was the first person to be elected to that office! Meaning while we edited the fact such that UK -&gt; US, the model still has its context about the US governing structure.</p> <p>While these results showcase intriguing potential, they remain a proof-of-concept. Establishing whether SAEs can truly serve as a reliable method for knowledge erasure—or for broader knowledge editing—requires rigorous evaluation. Future work should systematically test their precision in editing knowledge without unintended side effects on model performance.</p>]]></content><author><name>Yoav Gur Arieh</name></author><category term="NLP"/><category term="SAEs"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Can we leverage SAEs to effectively erase knowledge from LLMs in a targeted way?]]></summary></entry></feed>