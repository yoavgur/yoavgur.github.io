<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://yoavgur.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yoavgur.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-17T16:28:50+00:00</updated><id>https://yoavgur.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Using Sparse Autoencoders for Knowledge Erasure</title><link href="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/" rel="alternate" type="text/html" title="Using Sparse Autoencoders for Knowledge Erasure"/><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/sae-knowledge-erasure</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/"><![CDATA[<h4 id="introduction">Introduction</h4> <p>Sparse autoencoders (SAEs) have recently been the talk of the town in some circles of interpretability research, and are currently one of the more popular methods for unsupervised feature disentanglement. That is, they can enable us to study and use a more granular and hopefully monosemantic unit in an LLM.</p> <p>This has made them a particularly intriguing method for influencing LLMs in targeted ways, such as steering. Motivated by this, we explored their potential for <strong>knowledge erasure</strong>, aiming to determine whether they could support the selective removal of targeted information while maintaining the overall utility of a model.</p> <h4 id="knowledge-erasure">Knowledge Erasure</h4> <p>For simplicity, we use the simplistic setting of fact triplets ($s$, $r$, $o$) where $s$ is the subject, $r$ is the relation, and $o$ is the object. In this setting, our goal would be to affect our language model’s ability to generate tokens related to $o$ when prompted with $s$ and $r$. For example, when prompted with the sentence “<em>Barack Obama</em> (subject) <em>was born in</em> (relation)”, we want to make it so the language model in question won’t answer <em>Hawaii</em> (object).</p> <p>To do this, we’d hope to find an SAE feature that activates when prompted with the subject and relation, and which then promotes the token corresponding to the object. The challenge now is finding such a feature. We’ll use Gemma-2-2B as our target model for these experiments.</p> <h4 id="how-to-find-sae-knowledge-features">How to Find SAE Knowledge Features</h4> <p>To find the features of interest, we first need to know where to look. The way LLMs answer knowledge related queries in our setting is detailed in the paper <em>Dissecting Recall of Factual Associations in Auto-Regressive Language Models</em><d-cite key="geva-etal-2023-dissecting"></d-cite>. There, they show that the models follow three general steps:</p> <ol> <li>Subject enrichment, where the hidden representation of the final token of the subject is enriched by MLP layers with information relating to it. For the subject <em>Barack Obama</em>, the token <code class="language-plaintext highlighter-rouge">Obama</code> would be enriched with information like his birth date, where he went to college, and of course, where he was born.</li> <li>The relation information is propagated from the relation tokens to the final token.</li> <li>The relation information in the final token is used to extract the relevant information from the last subject token to the final token - i.e. in our example the token <code class="language-plaintext highlighter-rouge">Hawaii</code> would be extracted from the hidden representation in the <code class="language-plaintext highlighter-rouge">Obama</code> token position, and moved to the final token position.</li> </ol> <p>That means that a good way to erase the knowledge we want to erase, is to find which MLP SAE features are responsible for enriching the subject token with the information we want to erase.</p> <p>So now we know where to look - MLP SAE features that activate on the last token position of the subject token. But how do we know which one of these features?</p> <p>We could use existing feature descriptions, available for Gemma on Neuronpedia, to isolate ones that look like they relate. But as we showed in our paper<d-cite key="gurarieh2025enhancingautomatedinterpretabilityoutputcentric"></d-cite>, the methods used for generating these descriptions are mostly related to what activates a feature and not what it does once activated. Therefore, for each feature we can use vocabulary projection on the feature’s SAE decode column, and look to see if there are relevant tokens there - i.e. ones related to the object in our fact. Indeed, when looking at all MLP SAE features that activate for the token <code class="language-plaintext highlighter-rouge">Obama</code>, we see that feature 4999 in layer 6 seems to encode for the concept of the USA, and that <code class="language-plaintext highlighter-rouge">Hawaii</code> appears in the top 100 tokens of its vocabulary projection. This seems to be the feature we’re looking for!</p> <h4 id="erasing-the-knowledge---ablating-the-feature">Erasing the Knowledge - Ablating the Feature</h4> <p>Now that we have a method for finding the relevant features, we just have to prevent the feature from firing, or, ablating it. Practically, this means overriding its value so it’ll always be zero, but it’s been found that this doesn’t always work well enough<d-cite key="farrell2024applying"></d-cite>, and so instead we’ll try to set it to negative values.</p> <p>Now if we ablate the feature we found earlier that encodes for the concept of the USA, and prompt the model to answer where Barack Obama was born, funnily enough it answers <strong>Kenya</strong>! Looks like we might have accidentally made our model a birther…</p> <p>And these results seem to replicate across many facts!</p> <table> <thead> <tr> <th>Prompt</th> <th>Original Generation</th> <th>Ablated Feature</th> <th>New Generation</th> </tr> </thead> <tbody> <tr> <td>Barack Obama was born in</td> <td><strong>Hawaii</strong>, but he was raised in Indonesia. He was a student at Occidental.</td> <td>US (<code class="language-plaintext highlighter-rouge">6/4999</code>)</td> <td>1964 in the small town of <strong>Kinyanya, Kenya</strong>.</td> </tr> <tr> <td>George Bush was the governor of</td> <td><strong>Texas</strong> when he was elected president.</td> <td>Texas (<code class="language-plaintext highlighter-rouge">7/10671</code>)</td> <td><strong>Florida</strong> when he was elected president.</td> </tr> <tr> <td>The Dalai Lama is a spiritual leader from</td> <td><strong>Tibet</strong>. He is the 14th Dalai Lama</td> <td>Tibet (<code class="language-plaintext highlighter-rouge">17/8560</code>)</td> <td><strong>India</strong>. He is a Nobel Peace Prize winner.</td> </tr> <tr> <td>Winston Churchill was the Prime Minister of</td> <td>the <strong>United Kingdom</strong> during World War II. He was a great leader and a …</td> <td>UK (<code class="language-plaintext highlighter-rouge">14/8456</code>)</td> <td>the <strong>United States</strong> during the Second World War. He was the first person to be elected to the office of Prime Minister in the U.S.</td> </tr> <tr> <td>Thomas Jefferson wrote the</td> <td><strong>Declaration of Independence</strong> in 1776. He was the third president …</td> <td>Founding Fathers (<code class="language-plaintext highlighter-rouge">18/7722</code>)</td> <td><strong>the following</strong> letter to the editor of the New York Journal.</td> </tr> <tr> <td>The Taj Mahal is located in</td> <td>the country of <strong>India</strong>, it is a mausoleum build by the Mughal emperor Sha Jahan</td> <td>India (<code class="language-plaintext highlighter-rouge">4/16258</code>)</td> <td>the country of <strong>the United States</strong>. It is a monument that is dedicated to the memory …</td> </tr> <tr> <td>The Burj Khalifa is located in</td> <td><strong>Dubai</strong>, United Arab Emirates.</td> <td>Dubai (<code class="language-plaintext highlighter-rouge">14/6856</code>)</td> <td><strong>Chicago</strong>, Illinois.</td> </tr> <tr> <td>The Wright brothers invented</td> <td>the <strong>airplane</strong> in 1903. The first flight was a …</td> <td>Flight (<code class="language-plaintext highlighter-rouge">4/3225</code>)</td> <td>the concep of the <strong>car</strong>. They were the first to use the car as a …</td> </tr> <tr> <td>Marie Antoinette was queen of</td> <td><strong>France</strong> from 1774 to 1792.</td> <td>France (<code class="language-plaintext highlighter-rouge">18/11591</code>)</td> <td><strong>the United Kingdom</strong> from 1743 to 1745.</td> </tr> <tr> <td>Marie Curie discovered the element</td> <td><strong>Radium</strong> in 1898 She was awarded the Nobel Prize in Physics in 1903 …</td> <td>Nuclear (<code class="language-plaintext highlighter-rouge">16/2072</code>)</td> <td><strong>Platinum</strong> in 1800. It was named after the Greek goddess …</td> </tr> </tbody> </table> <p><br/></p> <h4 id="conclusion-limitations-and-future-directions">Conclusion, Limitations and Future Directions</h4> <p>Sparse autoencoders present a compelling framework for knowledge erasure by disentangling and selectively manipulating learned representations. We showed that this can enable us to perform targeted erasures, removing the undesirable knowledge and leaving the model still capable. For example, when we ablated the UK feature for Winston Churchill, the model continued the prompt saying that he was the prime minister of the US. But, it also added that it was during the Second World War (correct period) and that he was the first person to be elected to that office! Meaning while we edited the fact such that UK -&gt; US, the model still has its context about the US governing structure.</p> <p>While these results showcase intriguing potential, they remain a proof-of-concept. Establishing whether SAEs can truly serve as a reliable method for knowledge erasure—or for broader knowledge editing—requires rigorous evaluation. Future work should systematically test their precision in editing knowledge without unintended side effects on model performance.</p>]]></content><author><name>Yoav Gur Arieh</name></author><category term="NLP"/><category term="SAEs"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Can we leverage SAEs to effectively erase knowledge from LLMs in a targeted way?]]></summary></entry></feed>