<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://yoavgur.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yoavgur.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-03T06:58:07+00:00</updated><id>https://yoavgur.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context</title><link href="https://yoavgur.github.io/blog/2025/mixing-mechs/" rel="alternate" type="text/html" title="Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context"/><published>2025-09-29T00:00:00+00:00</published><updated>2025-09-29T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/mixing-mechs</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/mixing-mechs/"><![CDATA[<d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#three-mechanisms">Three Mechanisms For Binding and Retrieval</a></div> <div><a href="#results-and-analyses">Results and Analyses</a></div> <div><a href="#causal-model">A Simple Model for Simulating Entity Retrieval In-Context</a></div> <div><a href="#free-form">Introducing Free Form Text Into the Task</a></div> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#interactive">Interactive Figure</a></div> </nav> </d-contents> <p><strong>TL;DR</strong>: Entity binding in LMs is crucial for reasoning in LMs. Prior work established a <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism underlying binding, but we find that it breaks down in complex settings. We uncover two additional mechanisms—<mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark>—that drive model behavior.</p> <p><a href="#interactive">Jump to the interactive demo below</a></p> <figure style="display: flex; flex-direction: column; align-items: center; width: 100%; max-width: 1200px; margin-left: auto; margin-right: auto;"> <div style="width: 100%; max-width: min(150%, 1200px); margin-left: auto; margin-right: auto;" data-binding-demo="" data-mode="animated" data-initial-n="10" data-initial-ip="4" data-initial-il="4" data-initial-ir="4" data-initial-target="2" data-show-title="false" data-show-sentence="true" data-animation-sequence="[{&quot;iL&quot;:4,&quot;delay&quot;:500},{&quot;iR&quot;:4,&quot;delay&quot;:1500}, {&quot;iL&quot;:10,&quot;delay&quot;:500}, {&quot;iR&quot;:8,&quot;delay&quot;:1500}]"> </div> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem;"> <strong>Figure 1</strong>: A list of entities that need to be bound together is provided to the model. We see the model's (real collected) token probabilities, first when setting the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> (pos) / <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> (lex) / <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> (ref) mechanisms to point to the same entity, and then when pointing them at different entities. Interactive version <a href="#interactive">here</a>. </figcaption> </figure> <h3 id="introduction" style="scroll-margin-top: 80px;">Introduction</h3> <p>Language models (LMs) are known for their ability to perform in-context reasoning, and fundamental to this capability is the task of connecting related entities in a text—known as <em>binding</em>—to construct a representation of context that can be queried for next token prediction. For example, to represent the text <em>Pete loves jam and Ann loves pie</em>, an LM will bind <em>Pete</em> to <em>jam</em> and <em>Ann</em> to <em>pie</em>. This enables the LM to answer questions like <em>Who loves pie?</em> by querying the bound entities to retrieve the answer (<em>Ann</em>). The prevailing view is that LMs retrieve bound entities using a <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism<d-cite key="dai-etal-2024-representational"></d-cite><d-cite key="prakash2024finetuning"></d-cite><d-cite key="prakash2025languagemodelsuselookbacks"></d-cite>, where the query entity (<em>pie</em>) is used to determine the in-context position of <em>Ann loves pie</em>—in this case, the second clause after <em>Pete loves jam</em>—which is dereferenced to retrieve the answer <em>Ann</em>.</p> <p>However, we show that the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism becomes unreliable for middle positions in long lists of entity groups, echoing the <em>lost-in-the-middle</em> effect<d-cite key="liu-etal-2024-lost"></d-cite>. To compensate for this, LMs supplement the positional mechanism with a <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> mechanism, where the query entity (<em>pie</em>) is used to retrieve its bound counterpart (<em>Ann</em>), and a <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanism, where the queried entity (<em>Ann</em>) is retrieved with a direct pointer that was previously retrieved via the query entity (<em>pie</em>). See Figure <a href="#figure-2">2</a> for illustration.</p> <p>In this post we describe exactly how LMs use the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms for entity binding and retrieval, which we validate across (1) nine models from the Llama, Gemma and Qwen model families (2-72B parameters), and (2) ten different binding tasks. These mechanisms and their interactions remain consistent across all of these, establishing a general account of how LMs bind and retrieve entities.</p> <figure id="figure-2" style="display: flex; flex-direction: column; align-items: center; border: none; margin: 0; padding: 0;"> <a href="/assets/img/mechs_fig1.png" target="_blank" style="display:block; max-width:100%;text-decoration:none!important;border-bottom:0!important;box-shadow:none!important; background-image:none!important;"> <img src="/assets/img/mechs_fig1.png" alt="Mechanisms Figure 1" style="display:block; max-width:100%; height:auto; cursor:zoom-in; border:0;"/> </a> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem; border: none;"> <strong>Figure 2</strong>: An illustration of the three mechanisms for retrieving bound entities in-context and how we isolate them. We find that as models process inputs with groups of entities: (A) binding information of three types—<mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark>, <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark>—is encoded in the entity tokens of each group, (B) this binding information is jointly used to retrieve entities in-context, and (C) it is possible to separate the three binding signals with counterfactual patching. </figcaption> </figure> <h3 id="three-mechanisms">Three Mechanisms For Binding and Retrieval</h3> <p>The prevailing view is that entities are bound and retrieved with a <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism <d-cite key="dai-etal-2024-representational"></d-cite><d-cite key="prakash2024finetuning"></d-cite><d-cite key="prakash2025languagemodelsuselookbacks"></d-cite>. However, since this mechanism fails to explain model behavior in more complex settings, we propose and test two alternatives for how LMs might implement binding. To do this, we design datasets with pairs of original and counterfactual inputs, such that each of the three proposed mechanisms makes a distinct prediction under an interchange intervention with the pair. This is illustrated in Figure <a href="#figure-2">2</a>. Our three hopythesized mechanisms are:</p> <ul> <li><mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>Positional</b></mark> - given a query entity (What does <strong>Tim</strong> love?), the model extracts the index of the entity group to which it belongs (<strong>4</strong>), and retrieves the queried entity (<strong>tea</strong>) from it. Thus, patching this binding information from the counterfactual (where the queried entity group index is <strong>2</strong>) to the original would make the model respond with the answer from entity group number 2 (<strong>jam</strong>).</li> <li><mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>Lexical</b></mark> - where instead of using the position of the query entity, the model uses its lexical content to retrieve the bound entity from the group containing the query entity. This is achieved by copying the lexical contents of the <strong>Tim</strong> token into the <strong>tea</strong> token position (illustrated in Figure <a href="#figure-2">2</a> with a key), which can then trigger the attention of the query token (<strong>Tim</strong>), enabling retrieval of the bound entity (<strong>tea</strong>). Thus, patching in this binding information from the counterfactual (where the query entity is <strong>Ann</strong>) to the original would make the model respond with the answer from the entity group with Ann (<strong>ale</strong>).</li> <li><mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>Reflexive</b></mark> - where each entity in an entity group also encodes a pointer that points directly to itself, which can then by copied to other entities in its entity group and dereferenced in order to retrieve its originating entity. This mechanism is needed because the lexical mechanism, as described previously, requires information being copied between entity tokens to bind them together. However, if we query the first token in an entity group (i.e. <strong>Who loves tea?</strong>), then this mechanism would be useless, since the causal attention mask forbids the lexical contents of <strong>tea</strong> being copied <em>backward</em> to <strong>Tim</strong>. Thus, reflexive binding information pointing to <strong>Tim</strong> and originating from it, is copied <em>forward</em> to the <strong>tea</strong> token position, which can then be retrieved using the query entity (Who loves <strong>tea</strong>?), and used to retrieve <strong>Tim</strong>. Thus, in Figure <a href="#figure-2">2</a>, patching in this binding information from the counterfactual (where the answer entity is <strong>pie</strong>) to the original would make the model respond with <strong>pie</strong>. Note that this behavior is identical to patching in the answer itself from the counterfactual to the original, but we disambiguate these two mechanisms in multiple subsequent experiments involving more elaborate datasets as well as attention knockouts<d-cite key="geva-etal-2023-dissecting"></d-cite>, confirming the existence of this mechanism. Note also that this binding information only points to the token from which it originated (using its lexical content). Therefore, patching this binding information to a prompt where the <strong>pie</strong> entity doesn’t exist would lead to the model not being able to use this mechanism for retrieval.</li> </ul> <figure id="figure-3" style="display: flex; flex-direction: column; align-items: center; border: none; margin: 0; padding: 0;"> <a href="/assets/img/u_plot_gemma-2-2b.png" target="_blank" style="display:block; max-width:100%;text-decoration:none!important;border-bottom:0!important;box-shadow:none!important; background-image:none!important;"> <img src="/assets/img/u_plot_gemma-2-2b.png" alt="Mechanisms Figure 1" style="display:block; max-width:100%; height:auto; cursor:zoom-in; border:0;"/> </a> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem; border: none;"> <strong>Figure 3</strong>: Results from interchange interventions on gemma-2-2b-it over a counterfactual dataset with three entities per group. Outputs predicted by the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms are shown in dark blue, green and orange. Here $t_{entity}$ is the target entity, i.e. the entity being queried in the counterfactual example. <b>Left</b>: Distribution of effects for three representative entity group indices (first, middle, and last) with $t_{\text{entity}}=3$. At layers 16–18, the last token position carries binding information used for retrieval. <b>Right</b>: Distribution of effects for all indices at layer 18 for $t_{\text{entity}} \in \{1,2,3\}$, i.e., the question can be about any of the three entities in each clause. A U-shaped curve emerges: first and last indices rely more on the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism, while middle indices rely more on the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms. In the full paper, we replicate these exact results across all nine models and ten binding tasks. </figcaption> </figure> <h3 id="results-and-analyses">Results and Analyses</h3> <p>In Figure <a href="#figure-3">3</a>, we see the results of our interchange interventions for gemma-2-2b-it (replicated for all other models in the paper). We also collect the mean output probabilities for each possible answer entity post patching, highlighting the entities pointed to by each of the mechanisms, to understand the interplay between the three mechanisms.</p> <h4 id="aggregating-binding-information">Aggregating Binding Information</h4> <p>We see in Figure <a href="#figure-3">3</a> that in layers 0-15, no binding information exists in the last token position, since patching it doesn’t have any effect on model behavior. In layers 19-25, the model has already retrieved the bound entity, since patching the last token position leads to the model responding with the answer from the counterfactual example. However, in layers 16-18 we see that the model has aggregated the binding information in the last token position, since patching it from the counterfactual to the original leads the model to respond with entities corresponding to the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms.</p> <h4 id="the-positional-mechanism-is-weak-and-diffuse-in-middle-positions">The Positional Mechanism is Weak and Diffuse in Middle Positions</h4> <p>Figure <a href="#figure-3">3</a> shows that the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism controls model behavior only when querying the first few and last entity groups. In middle entity groups, however, its effect becomes minimal, only accounting for ~20% of model behavior.</p> <p>In Figure <a href="#figure-4">4</a> we see the mean probabilities for each answer entity (in order of their entity group index, i.e. in order of appearance in context), fixing the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms to point to the first entity, and sliding the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism across all values. We see that the mechanism induces a strong concentrated distribution around the entity pointed to by the positional mechanism in the beginning and end, but that in middle entity groups its distribution becomes weak and diffuse. This indicates that the model struggles to use positional information to bind together entities in middle entity groups, making it unreliable as the sole mechanism for binding.</p> <figure id="figure-4"> <div data-binding-demo="" data-mode="animated" data-initial-n="10" data-initial-ip="1" data-initial-il="1" data-initial-ir="1" data-initial-target="1" data-show-title="false" data-show-sentence="false" data-animation-sequence="[{&quot;iP&quot;:1,&quot;delay&quot;:1500},{&quot;iP&quot;:2,&quot;delay&quot;:1500},{&quot;iP&quot;:3,&quot;delay&quot;:1500},{&quot;iP&quot;:4,&quot;delay&quot;:1500},{&quot;iP&quot;:5,&quot;delay&quot;:1500},{&quot;iP&quot;:6,&quot;delay&quot;:1500},{&quot;iP&quot;:7,&quot;delay&quot;:1500},{&quot;iP&quot;:8,&quot;delay&quot;:1500},{&quot;iP&quot;:9,&quot;delay&quot;:1500},{&quot;iP&quot;:10,&quot;delay&quot;:1500}]"> </div> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem;"> <strong>Figure 4</strong>: Mean probabilities for all answer entities, fixing the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms to point to the first entity, and sliding the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism across all values. We see that in early and late entity groups the positional mechanism induces a strong concentrated distribution, while for middle ones it becomes weak and diffuse, rendering it less robust. </figcaption> </figure> <p><br/></p> <h4 id="interplay-between-the-mechanisms">Interplay Between The Mechanisms</h4> <p>We see in Figure <a href="#figure-5">5</a> that, contrary to the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism, the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms induce one-hot distributions at their target entities. The distributions induced by the three mechanisms then interact with each other in shaping the model’s behavior, both boosting and suppressing each other. We see for example that when the lexical / reflexive mechanisms point at entities near the positional one, their probabilities increase dramatically, while simultaneously suppressing that of the entity pointed to by the positional mechanism.</p> <figure id="figure-5"> <div data-binding-demo="" data-mode="animated" data-initial-n="8" data-initial-ip="4" data-initial-il="1" data-initial-ir="8" data-initial-target="1" data-show-title="false" data-show-sentence="false" data-animation-sequence="[{&quot;iL&quot;:1,&quot;delay&quot;:1000},{&quot;iL&quot;:2,&quot;delay&quot;:1000},{&quot;iL&quot;:3,&quot;delay&quot;:1000},{&quot;iL&quot;:4,&quot;delay&quot;:1000},{&quot;iL&quot;:5,&quot;delay&quot;:1000},{&quot;iL&quot;:6,&quot;delay&quot;:1000},{&quot;iL&quot;:7,&quot;delay&quot;:1000},{&quot;iL&quot;:8,&quot;delay&quot;:1000}, {&quot;iR&quot;:1,&quot;delay&quot;:1000},{&quot;iR&quot;:2,&quot;delay&quot;:1000},{&quot;iR&quot;:3,&quot;delay&quot;:1000},{&quot;iR&quot;:4,&quot;delay&quot;:1000},{&quot;iR&quot;:5,&quot;delay&quot;:1000},{&quot;iR&quot;:6,&quot;delay&quot;:1000},{&quot;iR&quot;:7,&quot;delay&quot;:1000},{&quot;iR&quot;:8,&quot;delay&quot;:1000}]"> </div> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem;"> <strong>Figure 5</strong>: Interaction between the different mechanisms. We fix the entity pointed to by the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism, and slide the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms. We see that the latter two induce one-hot distributions, and interact with each other and the positional mechanism in additive and suppressive ways. </figcaption> </figure> <h3 id="causal-model">A Simple Model for Simulating Entity Retrieval In-Context</h3> <p>To formalize out observations about the dynamics between the three mechanisms and the position of the queried entity, we develop a high-level causal model<d-cite key="geiger2021causal"></d-cite> that approximates LM logits for next token prediction. We formalize this as a position-weighted mixture of terms for the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms. In accordance with the results in Figure <a href="#figure-5">5</a>, we model the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms as one-hot distributions that up-weight only entities pointed to by those mechanisms, while the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism is modeled as a gaussian distribution scaled by a single weight, with a standard deviation that is a quadratic function of the positional index. Formally:</p> <div style="overflow-x:auto;"> $$Y_i := \underbrace{w_{\mathrm{pos}} \cdot \mathcal{N}\left(i \mid i_P, \sigma(i_P)^2\right)}_{\text{positional mechanism}} + \underbrace{w_{\mathrm{lex}}\!\left[i_L\right]\cdot \mathbf{1}\!\left\{i = i_L\right\}}_{\text{lexical mechanism}} + \underbrace{w_{\mathrm{ref}}\!\left[i_R\right]\cdot \mathbf{1}\{i=i_R\}}_{\text{reflexive mechanism}}$$ </div> <p>Where $i_{P/L/R}$ are the entity group indices pointed by the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms respectively, and $\sigma(i_P) = \alpha (\frac{i_P}{n})^2 + \beta \frac{i_P}{n} + \gamma$. We learn $w_{pos},w_{lex},w_{ref},\alpha, \beta, \gamma$ from data.</p> <p>To train and evaluate our model, we collect the logit distributions per index combination, and average them into mean probability distributions by first applying a softmax over the entity group indices, and then take taking the mean. We calculate the loss using Jensen-Shannon divergence (chosen for its symmetry), and measure performance using Jensen-Shannon similarity (JSS), its complement, which ranges from 0 to 1. We also compare our model to the prevailing view (one-hot distribution at $i_P$), as well as various ablations of our model. Finally, we compare our model to an oracle variant, an upper bound where we replace the learned gaussian with the actual collected probabilities.</p> <figure id="table-1" style="width:100%;"> <div style="overflow-x:auto;"> <table style="width:100%; border-collapse:collapse;"> <thead> <tr> <th style="text-align:left;"><strong>Model</strong></th> <th style="text-align:center;"><strong>JSS ↑ $(t_e=1)$</strong></th> <th style="text-align:center;"><strong>JSS ↑ $(t_e=2)$</strong></th> <th style="text-align:center;"><strong>JSS ↑ $(t_e=3)$</strong></th> </tr> </thead> <tbody> <tr><td colspan="4" style="text-align:center;"><em>Comparing against the prevailing view</em></td></tr> <tr> <td>$\mathcal{M}(L_{\text{one-hot}}, R_{\text{one-hot}}, P_{\text{Gauss}})$</td> <td style="text-align:center;"><strong>0.95</strong></td> <td style="text-align:center;"><strong>0.96</strong></td> <td style="text-align:center;"><strong>0.94</strong></td> </tr> <tr> <td>$\mathcal{P}_{\text{one-hot}}$ (prevailing view)</td> <td style="text-align:center;">0.42</td> <td style="text-align:center;">0.46</td> <td style="text-align:center;">0.45</td> </tr> <tr><td colspan="4" style="text-align:center;"><em>Modifying the positional mechanism</em></td></tr> <tr> <td>$\mathcal{M}$ w/ $P_{\text{oracle}}$</td> <td style="text-align:center;">0.96</td> <td style="text-align:center;">0.98</td> <td style="text-align:center;">0.96</td> </tr> <tr> <td>$\mathcal{M}$ w/ $P_{\text{one-hot}}$</td> <td style="text-align:center;">0.86</td> <td style="text-align:center;">0.85</td> <td style="text-align:center;">0.85</td> </tr> <tr><td colspan="4" style="text-align:center;"><em>Ablating the three mechanisms</em></td></tr> <tr> <td>$\mathcal{M} \setminus \{P_{\text{Gauss}}\}$</td> <td style="text-align:center;">0.67</td> <td style="text-align:center;">0.68</td> <td style="text-align:center;">0.67</td> </tr> <tr> <td>$\mathcal{M} \setminus \{L_{\text{one-hot}}\}$</td> <td style="text-align:center;">0.94</td> <td style="text-align:center;">0.91</td> <td style="text-align:center;">0.75</td> </tr> <tr> <td>$\mathcal{M} \setminus \{R_{\text{one-hot}}\}$</td> <td style="text-align:center;">0.69</td> <td style="text-align:center;">0.87</td> <td style="text-align:center;">0.92</td> </tr> <tr> <td>$\mathcal{M} \setminus \{R_{\text{one-hot}}, L_{\text{one-hot}}\}$</td> <td style="text-align:center;">0.69</td> <td style="text-align:center;">0.84</td> <td style="text-align:center;">0.74</td> </tr> <tr> <td>$\mathcal{M} \setminus \{P_{\text{Gauss}}, R_{\text{one-hot}}\}$</td> <td style="text-align:center;">0.12</td> <td style="text-align:center;">0.27</td> <td style="text-align:center;">0.48</td> </tr> <tr> <td>$\mathcal{M} \setminus \{P_{\text{Gauss}}, L_{\text{one-hot}}\}$</td> <td style="text-align:center;">0.55</td> <td style="text-align:center;">0.41</td> <td style="text-align:center;">0.20</td> </tr> <tr> <td>Uniform</td> <td style="text-align:center;">0.44</td> <td style="text-align:center;">0.57</td> <td style="text-align:center;">0.49</td> </tr> </tbody> </table> </div> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem;"> <strong>Table 1:</strong> Jensen-Shannon similarity (JSS) results for training our full model $\mathcal{M}$, in addition to variants, baselines and ablations. We show results for $t_e \in [3]$, i.e. when querying the first, second and third entities in an entity group, respectively. </figcaption> </figure> <p><br/></p> <p>We see in Table <a href="#table-1">1</a> that our model achieves near perfect results (0.95), only slightly below the oracle variant (0.97). We also see that the model representing the prevailing view achieves much worse results (0.44), well below even a uniform distribution (0.5). Finally, we can see when each component of our model is important for modeling the LM’s behavior: when querying the first entity in a group, the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> mechanism is not crucial for modeling the LM’s behavior, while when querying the last entity the <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> one isn’t, in line with our hypothesis.</p> <h3 id="free-form">Introducing Free Form Text Into the Task</h3> <p>To test our model’s generalization to more realistic inputs, we modify our binding tasks such that they include filler sentences between each entity group. To this end, we create 1,000 filler sentences that are “entity-less”, meaning they do not contain sequences that signal the need to track or bind entities, e.g. “Ann loves ale, <em>this is a known fact</em>, Joe loves jam, <em>this logic is easy to follow</em>…”. This enables us to evaluate entity binding in a more naturalistic setting, containing much more noise and longer sequences. We evaluate different levels of padding by interleaving the entity groups with an increasing number of filler sentences, for a maximum of 500 tokens between each entity group.</p> <figure id="figure-6" style="display: flex; flex-direction: column; align-items: center; border: none; margin: 0; padding: 0;"> <a href="/assets/img/filler_trends_stacked.png" target="_blank" style="display:block; max-width:100%;text-decoration:none!important;border-bottom:0!important;box-shadow:none!important; background-image:none!important;"> <img src="/assets/img/filler_trends_stacked.png" alt="Mechanisms Figure 1" style="display:block; max-width:100%; height:auto; cursor:zoom-in; border:0;"/> </a> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem; border: none;"> <strong>Figure 6</strong>: Distribution of effects as padding is increased, showing the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism strengthens at the expense of the <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> mechanism. </figcaption> </figure> <p><br/></p> <p>The results, shown in Figure <a href="#figure-6">6</a> and <a href="#figure-7">7</a>, show that our model at first remains remarkably consistent in more naturalistic settings, across even a ten-fold increase in the number of tokens. However, as the amount of filler tokens increases, we see that the magnitude of the mechanisms’ effects changes. The <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> mechanism declines in its effect, while the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> and mixed effects slightly increase. We can also see that the mixed effect remains distributed around the positional index, but that it slowly becomes more diffuse. Thus, when padding with 10,000 tokens, we get that other than the first entity group, the positional information becomes nearly non-existent for the first half of entity tokens, while remaining stronger in the latter half. This suggests that a weakening lexical mechanism relative to an increasingly noisy positional mechanism might be a mechanistic explanation of the “lost-in-the-middle” effect<d-cite key="liu-etal-2024-lost"></d-cite>.</p> <figure id="figure-7" style="display: flex; flex-direction: column; align-items: center; border: none; margin: 0; padding: 0;"> <a href="/assets/img/confusion_matrix_padding.png" target="_blank" style="display:block; max-width:100%;text-decoration:none!important;border-bottom:0!important;box-shadow:none!important; background-image:none!important;"> <img src="/assets/img/confusion_matrix_padding.png" alt="Mechanisms Figure 1" style="display:block; max-width:100%; height:auto; cursor:zoom-in; border:0;"/> </a> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem; border: none;"> <strong>Figure 7</strong>: Confusion matrix between the model's predicted index and the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> index patched in from the counterfactual. This gets increasingly fuzzy for early tokens as padding is increased. </figcaption> </figure> <h3 id="conclusion">Conclusion</h3> <p>In our work, we challenge the prevailing view that LMs retrieve bound entities purely with a <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism. We find that while the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> mechanism is effective for entities introduced at the beginning or end of context, it becomes diffuse and unreliable in the middle. We show that in practice, LMs rely on a mixture of three mechanisms: <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark>, and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark>. The lexical and reflexive mechanisms provide sharper signals that enable the model to correctly bind and retrieve entities throughout. We validate our findings across 9 models ranging from 2B to 72B parameters, and 10 binding tasks, establishing a general account of how LMs retrieve bound entities.</p> <hr/> <h3 id="interactive" style="scroll-margin-top: 80px;"> Interactive Figure </h3> <p>Here we provide an interactive figure, showing the mean output probabilities (gemma-2-2b-it) over the possible answer entities contingent on which entities are pointed to by the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark>, <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> and <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms (pos, lex and ref respectively). You can control the number of entity groups in the context (n), as well as which entity in a group is queried (target). See the <a href="#introduction">full blog post</a> or our paper to understand more about how LMs perform binding and retrieval.</p> <figure> <div data-binding-demo="" data-mode="interactive" data-initial-n="10" data-initial-ip="4" data-initial-il="10" data-initial-ir="8" data-initial-target="2" data-show-title="true" data-show-sentence="true"></div> <figcaption style="font-size: 0.9em; color: #6c757d; margin-top: 0.5rem;"> Figure 7: Interactive version where you can set how many entities exist, and the entities pointed to by the <mark style="background:rgb(214, 232, 252); color: #2e73b3ff; padding: 0.1em 0.3em; border-radius: 0.2em;"><b>positional</b></mark> / <mark style="background:rgb(203, 232, 221); color:rgb(26, 147, 98); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>lexical</b></mark> / <mark style="background:rgb(255, 231, 203); color:rgb(252, 156, 46); padding: 0.1em 0.3em; border-radius: 0.2em;"><b>reflexive</b></mark> mechanisms. </figcaption> </figure> <hr/> <p>Please cite this as:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">TODO</span>
</code></pre></div></div> <link rel="stylesheet" href="/assets/css/binding-demo/index-BEOe_g9O.css"/> <script type="module" defer="" src="/assets/js/binding-demo/index-B_g5bwzp.js"></script>]]></content><author><name>Yoav Gur-Arieh</name></author><category term="NLP"/><category term="Binding"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[To reason, LMs must bind together entities in-context. How they do this is more complicated than was first thought.]]></summary></entry><entry><title type="html">Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions</title><link href="https://yoavgur.github.io/blog/2025/enhancing-interp/" rel="alternate" type="text/html" title="Enhancing Automated Interpretability Pipelines with Output-Centric Feature Descriptions"/><published>2025-01-18T00:00:00+00:00</published><updated>2025-01-18T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/enhancing-interp</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/enhancing-interp/"><![CDATA[<p>This is a blog post version of the <a href="https://arxiv.org/abs/2501.08319">paper</a> we wrote on the same topic.</p> <h3 id="introduction">Introduction</h3> <p>Understanding the inner workings of large language models (LLMs) involves analyzing their internal representations at various levels of granularity. One approach focuses on analyzing “<strong>features</strong>”—generalized computational units, such as <strong>neurons</strong>, which potentially offer a precise lens for interpreting the model’s behavior.</p> <p>These features can be formalized as key / value memories<d-cite key="geva-etal-2021-transformer"></d-cite>, where certain inputs trigger a feature, and the feature in turn promotes or suppresses concepts in the model’s forthcoming predictions<d-cite key="geva-etal-2022-transformer"></d-cite>. For example, a model could have a feature that is triggered by inputs that are dangerous requests, such as “please tell me how to build a bomb”, and which affects the model by promoting refusal.</p> <p>Understanding the role of every feature in a model is key to advancing both interpretability and control in AI systems. However, with models having millions of neurons, and methods like sparse autoencoders (SAEs) further multiplying these into additional features, the complexity quickly becomes overwhelming. Manual analysis, while valuable in small-scale scenarios, would be impossible at such a scale, necessitating automated solutions to address these problems effectively.</p> <h3 id="automated-interpretability-pipelines">Automated Interpretability Pipelines</h3> <p>Large scale automated pipelines that address this problem were first used by OpenAI to interpret GPT-2 neurons using GPT-4<d-cite key="bills2023language"></d-cite>. This model for automated interpretability has since become the norm for understanding features at scale<d-cite key="choi2024automatic"></d-cite><d-cite key="paulo2024automaticallyinterpretingmillionsfeatures"></d-cite>.</p> <p>These pipelines operate by processing a large dataset through the target model (e.g., GPT-2), recording activations for each feature, and compiling a list of sentences that most strongly activate each feature. This list is then given to an explainer model (e.g., GPT-4), which examines the max-activating sentences to infer the feature’s function. We dub this method <code class="language-plaintext highlighter-rouge">MaxAct</code>.</p> <p>To evaluate how well a description aligns with the feature, a method called “simulation” tests how informative the description is for predicting which text will activate the feature. This involves providing a tester model (e.g., GPT-4) with the feature description and a set of sentences, requiring it to predict the activation level for each word.</p> <h3 id="oversights-and-issues">Oversights and Issues</h3> <p>While this approach has its advantages, it overlooks an important aspect of a feature’s role. Namely, it focuses solely on understanding “<strong>what inputs activate the feature</strong>” while neglecting to address “<strong>how the feature influences the model</strong>” once activated. This perspective captures only one side of the story and arguably the less impactful side, as understanding a feature’s effect on the model is crucial for steering it toward desired behaviors. This method is also very expensive, since processing a huge dataset and recording all of its activations can be very costly.</p> <p>To address this, we propose two output-centric methods that are both more efficient (requiring either only a few model runs, or no runs), and are better at capturing a feature’s causal effects on the model. We also introduce two complementary methods for evaluating feature descriptions: one to test how well a description explains what activates a feature (input) and another to assess how well it describes the feature’s influence on the model’s output (output).</p> <h3 id="focusing-on-the-output">Focusing on the Output</h3> <p>The first method, dubbed <code class="language-plaintext highlighter-rouge">VocabProj</code>, is simply applying vocabulary projection (a.k.a. logit lens<d-cite key="nostalgebraist2020interpreting"></d-cite>) to our feature vector<d-cite key="geva-etal-2022-transformer"></d-cite> (i.e. the relevant row vector in the MLP out matrix, or the SAE decode matrix). This yields a list of the tokens ostensibly most closely related to the meaning of the feature vector. We can then pass this list to an explainer model (e.g. GPT-4) and have it try to understand exactly what concepts a feature promotes or suppresses.</p> <p>The second method, dubbed <code class="language-plaintext highlighter-rouge">TokenChange</code>, takes a more causal approach. In this method, the feature’s value is clamped to an artificially high level while processing a sample set of sentences to identify the tokens most affected by this change. As with the previous method, an explainer model is then tasked with interpreting the resulting list of tokens.</p> <p>These two methods are inexpensive to run, and provide us with insights regarding how a feature actually affects the model. Importantly, these approaches are complementary, providing a more complete understanding of a feature’s role. For instance, consider the MLP SAE feature <code class="language-plaintext highlighter-rouge">19/5635</code> from Gemma-2 2B. The inputs that most activate this feature are ‘‘<em>Inauguration</em>”, “<em>Election</em>”, “<em>Race</em>”, “<em>funeral</em>” and “<em>opening</em>”, suggesting a connection to events. Meanwhile, the tokens most associated with its outputs are “<em>week</em>”, “<em>weekend</em>”, “<em>day</em>”, “<em>month</em>” and “<em>year</em>”, pointing to time measurements. Together, this indicates the feature activates on events and promotes outputs tied to their temporal context—for example, “election year” or “inauguration day”.</p> <h3 id="evaluating-descriptions">Evaluating Descriptions</h3> <p>To evaluate these feature descriptions, we propose an input-based evaluation and an output based one. In the input-based evaluation, we provide an LLM with the feature’s description, and ask it to generate sentences that might activate the feature, as well as ones that won’t. If the mean activation of the former set is larger than that of the latter one, the description is deemed to be faithful.</p> <p>In the output-based evaluation, we amplify the target feature and observe its influence on the model’s generated text. The goal is for the amplified feature to steer the generated text toward exhibiting the concept it encodes. For example, amplifying a feature associated with ‘games’ should prompt the model to generate text related to games. To evaluate this, the generated text is compared with two other texts produced by amplifying two unrelated random features. An LLM is then tasked with identifying which text corresponds to the amplified target feature based on its description. If it answers correctly, the description is deemed to be faithful.</p> <h3 id="results">Results</h3> <p>Unsurprisingly, each method excels in its own category. The input-centric method <code class="language-plaintext highlighter-rouge">MaxAct</code> outperforms the output-centric ones on the input-based metric, while the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> on the output-based metric.</p> <p>Remarkably, an ensemble of the three methods performs better than all individual methods on both metrics! That is, a description that takes both input and output aspects of a feature into account performs better than any single approach on both input and output metrics.</p> <div class="l-page" style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/enhancing_results.html" frameborder="0" scrolling="no" height="400px" width="760px" style="border: 1px dashed grey;"></iframe> </div> <h3 id="conclusion">Conclusion</h3> <p>We showed that the output-centric methods <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> consistently outperform <code class="language-plaintext highlighter-rouge">MaxAct</code> in output-based evaluations, highlighting the limitations of <code class="language-plaintext highlighter-rouge">MaxAct</code> in capturing the causal role of features. Additionally, these methods are significantly more computationally efficient and often approach <code class="language-plaintext highlighter-rouge">MaxAct</code>’s performance on input-based metrics, making them a practical and cost-effective alternative. Finally, we showed how <code class="language-plaintext highlighter-rouge">VocabProj</code> and <code class="language-plaintext highlighter-rouge">TokenChange</code> enhance automated interpretability pipelines by delivering more faithful feature descriptions across both evaluation dimensions.</p> <p>For a demonstration of how understanding a feature translates into real-world applications, have a look at <a href="https://yoav.ml/blog/2025/sae-knowledge-erasure/">this</a> blog post showcasing how it can facilitate knowledge erasure in LLMs. For more details about this work you can read our <a href="https://arxiv.org/abs/2501.08319">paper</a>.</p>]]></content><author><name>Yoav Gur Arieh</name></author><category term="NLP"/><category term="SAEs"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[This is a blog post version of the paper we wrote on the same topic.]]></summary></entry><entry><title type="html">Using Sparse Autoencoders for Knowledge Erasure</title><link href="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/" rel="alternate" type="text/html" title="Using Sparse Autoencoders for Knowledge Erasure"/><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><id>https://yoavgur.github.io/blog/2025/sae-knowledge-erasure</id><content type="html" xml:base="https://yoavgur.github.io/blog/2025/sae-knowledge-erasure/"><![CDATA[<h4 id="introduction">Introduction</h4> <p>Sparse autoencoders (SAEs) have recently been the talk of the town in some circles of interpretability research, and are currently one of the more popular methods for unsupervised feature disentanglement. That is, they can enable us to study and use a more granular and hopefully monosemantic unit in an LLM.</p> <p>This has made them a particularly intriguing method for influencing LLMs in targeted ways, such as steering. Motivated by this, we explored their potential for <strong>knowledge erasure</strong>, aiming to determine whether they could support the selective removal of targeted information while maintaining the overall utility of a model.</p> <h4 id="knowledge-erasure">Knowledge Erasure</h4> <p>For simplicity, we use the simplistic setting of fact triplets ($s$, $r$, $o$) where $s$ is the subject, $r$ is the relation, and $o$ is the object. In this setting, our goal would be to affect our language model’s ability to generate tokens related to $o$ when prompted with $s$ and $r$. For example, when prompted with the sentence “<em>Barack Obama</em> (subject) <em>was born in</em> (relation)”, we want to make it so the language model in question won’t answer <em>Hawaii</em> (object).</p> <p>To do this, we’d hope to find an SAE feature that activates when prompted with the subject and relation, and which then promotes the token corresponding to the object. The challenge now is finding such a feature. We’ll use Gemma-2-2B as our target model for these experiments.</p> <h4 id="how-to-find-sae-knowledge-features">How to Find SAE Knowledge Features</h4> <p>To find the features of interest, we first need to know where to look. The way LLMs answer knowledge related queries in our setting is detailed in the paper <em>Dissecting Recall of Factual Associations in Auto-Regressive Language Models</em><d-cite key="geva-etal-2023-dissecting"></d-cite>. There, they show that the models follow three general steps:</p> <ol> <li>Subject enrichment, where the hidden representation of the final token of the subject is enriched by MLP layers with information relating to it. For the subject <em>Barack Obama</em>, the token <code class="language-plaintext highlighter-rouge">Obama</code> would be enriched with information like his birth date, where he went to college, and of course, where he was born.</li> <li>The relation information is propagated from the relation tokens to the final token.</li> <li>The relation information in the final token is used to extract the relevant information from the last subject token to the final token - i.e. in our example the token <code class="language-plaintext highlighter-rouge">Hawaii</code> would be extracted from the hidden representation in the <code class="language-plaintext highlighter-rouge">Obama</code> token position, and moved to the final token position.</li> </ol> <p>That means that a good way to erase the knowledge we want to erase, is to find which MLP SAE features are responsible for enriching the subject token with the information we want to erase.</p> <p>So now we know where to look - MLP SAE features that activate on the last token position of the subject token. But how do we know which one of these features?</p> <p>We could use existing feature descriptions, available for Gemma on Neuronpedia, to isolate ones that look like they relate. But as we showed in our paper<d-cite key="gurarieh2025enhancingautomatedinterpretabilityoutputcentric"></d-cite>, the methods used for generating these descriptions are mostly related to what activates a feature and not what it does once activated. Therefore, for each feature we can use vocabulary projection on the feature’s SAE decode column, and look to see if there are relevant tokens there - i.e. ones related to the object in our fact. Indeed, when looking at all MLP SAE features that activate for the token <code class="language-plaintext highlighter-rouge">Obama</code>, we see that feature 4999 in layer 6 seems to encode for the concept of the USA, and that <code class="language-plaintext highlighter-rouge">Hawaii</code> appears in the top 100 tokens of its vocabulary projection. This seems to be the feature we’re looking for!</p> <h4 id="erasing-the-knowledge---ablating-the-feature">Erasing the Knowledge - Ablating the Feature</h4> <p>Now that we have a method for finding the relevant features, we just have to prevent the feature from firing, or, ablating it. Practically, this means overriding its value so it’ll always be zero, but it’s been found that this doesn’t always work well enough<d-cite key="farrell2024applying"></d-cite>, and so instead we’ll try to set it to negative values.</p> <p>Now if we ablate the feature we found earlier that encodes for the concept of the USA, and prompt the model to answer where Barack Obama was born, funnily enough it answers <strong>Kenya</strong>! Looks like we might have accidentally made our model a birther…</p> <p>And these results seem to replicate across many facts!</p> <table> <thead> <tr> <th>Prompt</th> <th>Original Generation</th> <th>Ablated Feature</th> <th>New Generation</th> </tr> </thead> <tbody> <tr> <td>Barack Obama was born in</td> <td><strong>Hawaii</strong>, but he was raised in Indonesia. He was a student at Occidental.</td> <td>US (<code class="language-plaintext highlighter-rouge">6/4999</code>)</td> <td>1964 in the small town of <strong>Kinyanya, Kenya</strong>.</td> </tr> <tr> <td>George Bush was the governor of</td> <td><strong>Texas</strong> when he was elected president.</td> <td>Texas (<code class="language-plaintext highlighter-rouge">7/10671</code>)</td> <td><strong>Florida</strong> when he was elected president.</td> </tr> <tr> <td>The Dalai Lama is a spiritual leader from</td> <td><strong>Tibet</strong>. He is the 14th Dalai Lama</td> <td>Tibet (<code class="language-plaintext highlighter-rouge">17/8560</code>)</td> <td><strong>India</strong>. He is a Nobel Peace Prize winner.</td> </tr> <tr> <td>Winston Churchill was the Prime Minister of</td> <td>the <strong>United Kingdom</strong> during World War II. He was a great leader and a …</td> <td>UK (<code class="language-plaintext highlighter-rouge">14/8456</code>)</td> <td>the <strong>United States</strong> during the Second World War. He was the first person to be elected to the office of Prime Minister in the U.S.</td> </tr> <tr> <td>Thomas Jefferson wrote the</td> <td><strong>Declaration of Independence</strong> in 1776. He was the third president …</td> <td>Founding Fathers (<code class="language-plaintext highlighter-rouge">18/7722</code>)</td> <td><strong>the following letter</strong> to the editor of the New York Journal.</td> </tr> <tr> <td>The Taj Mahal is located in</td> <td>the country of <strong>India</strong>, it is a mausoleum built by the Mughal emperor Sha Jahan</td> <td>India (<code class="language-plaintext highlighter-rouge">4/16258</code>)</td> <td>the country of <strong>the United States</strong>. It is a monument that is dedicated to the memory …</td> </tr> <tr> <td>The Burj Khalifa is located in</td> <td><strong>Dubai</strong>, United Arab Emirates.</td> <td>Dubai (<code class="language-plaintext highlighter-rouge">14/6856</code>)</td> <td><strong>Chicago</strong>, Illinois.</td> </tr> <tr> <td>The Wright brothers invented</td> <td>the <strong>airplane</strong> in 1903. The first flight was a …</td> <td>Flight (<code class="language-plaintext highlighter-rouge">4/3225</code>)</td> <td>the concept of the <strong>car</strong>. They were the first to use the car as a …</td> </tr> <tr> <td>Marie Antoinette was queen of</td> <td><strong>France</strong> from 1774 to 1792.</td> <td>France (<code class="language-plaintext highlighter-rouge">18/11591</code>)</td> <td><strong>the United Kingdom</strong> from 1743 to 1745.</td> </tr> <tr> <td>Marie Curie discovered the element</td> <td><strong>Radium</strong> in 1898 She was awarded the Nobel Prize in Physics in 1903 …</td> <td>Nuclear (<code class="language-plaintext highlighter-rouge">16/2072</code>)</td> <td><strong>Platinum</strong> in 1800. It was named after the Greek goddess …</td> </tr> </tbody> </table> <p><br/></p> <h4 id="conclusion-limitations-and-future-directions">Conclusion, Limitations and Future Directions</h4> <p>Sparse autoencoders present a compelling framework for knowledge erasure by disentangling and selectively manipulating learned representations. We showed that this can enable us to perform targeted erasures, removing the undesirable knowledge and leaving the model still capable. For example, when we ablated the UK feature for Winston Churchill, the model continued the prompt saying that he was the prime minister of the US. But, it also added that it was during the Second World War (correct period) and that he was the first person to be elected to that office! Meaning while we edited the fact such that UK -&gt; US, the model still has its context about the US governing structure.</p> <p>While these results showcase intriguing potential, they remain a proof-of-concept. Establishing whether SAEs can truly serve as a reliable method for knowledge erasure—or for broader knowledge editing—requires rigorous evaluation. Future work should systematically test their precision in editing knowledge without unintended side effects on model performance.</p>]]></content><author><name>Yoav Gur Arieh</name></author><category term="NLP"/><category term="SAEs"/><category term="Interpretability"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Can we leverage SAEs to effectively erase knowledge from LLMs in a targeted way?]]></summary></entry></feed>